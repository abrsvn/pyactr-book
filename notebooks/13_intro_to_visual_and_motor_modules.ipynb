{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](./images/colab-badge.png)](https://colab.research.google.com/github/abrsvn/pyactr-book/blob/master/notebooks/13_intro_to_visual_and_motor_modules.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax as a Cognitive Process: Left-corner parsing with visual \\& motor interfaces\n",
    "\n",
    "In the previous notebooks, we introduced and used several ACT-R modules and buffers:\n",
    "\n",
    "- the declarative memory module and its retrieval buffer\n",
    "- the procedural memory module and its goal buffer\n",
    "- the imaginal buffer\n",
    "\n",
    "These are core ACT-R modules and buffers, but focusing exclusively on them leads to solipsistic models that do not interact in any way with the environment.\n",
    "\n",
    "In this notebook, we are going to start changing that and introduce the vision and motor modules, which give us basic ways to be affected by and in turn affect the environment outside the mind.\n",
    "\n",
    "We will then leverage these input/output interfaces when we build a psycholinguistically realistic left-corner parser for the syntactic component of the linguistic representations we will model in this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The environment in ACT-R: modeling lexical decision tasks\n",
    "\n",
    "We will introduce ACT-R environments by modeling a simple lexical decision task.\n",
    "\n",
    "- modeling lexical decision tasks is a good stepping stone towards our goal of providing an end-to-end model of self-paced reading\n",
    "- by end-to-end, we mean a model of self-paced reading that includes both syntactic and semantic parsing (and therefore lexical retrieval of both syntactic and semantic information), and that importantly also has\n",
    "    - a suitable vision interface to model the way a human perceives the linguistic input incrementally presented on the screen, and\n",
    "    - a suitable motor interface to model the way a human self-paced reader interacts with the keyboard.\n",
    "\n",
    "In lexical decision tasks, participants perceive a string and decide whether that string is a word in their language. We will build an ACT-R model to simulate human behavior in this type of tasks.\n",
    "\n",
    "- the model will search a (virtual) screen and find a string of characters / word on the screen \n",
    "- if the word matches the (impoverished) lexicon of the model, the model will press the J key on its (virtual) keyboard; otherwise, it will press the F key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing ```pyactr``` and creating an environment.\n",
    "\n",
    "- the environment is just a (simulated) computer screen, and a pretty basic one at that: only plain text is supported\n",
    "- but that is enough for our purposes throughout this course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyactr as actr\n",
    "\n",
    "environment = actr.Environment(focus_position=(0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the class ```Environment``` is initialized, we can specify various parameters.\n",
    "\n",
    "- here, we only specify ```focus_position```, which indicates the position the eyes focus on when the simulation starts\n",
    "\n",
    "Two other parameters are:\n",
    "\n",
    "- ```simulated_screen_size```, which specifies the physical size of the screen we are simulating in cm (default: $50\\times 28$ cm)\n",
    "- ```viewing_distance```, which specifies the distance between the simulated participants eyes and the screen (default: $50$ cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the environment is initialized, we can initialize our ACT-R model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_decision = actr.ACTRModel(\n",
    "    environment=environment,\n",
    "    automatic_visual_search=False,\n",
    "    motor_prepared=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initialization is similar to what we used before except we specify environment-related arguments:\n",
    "\n",
    "- we state what environment the model / mind is interacting with\n",
    "- we set ```automatic_visual_search``` to ```False``` so that the model does not start searching the environment for input unless we specifically ask it to\n",
    "- we state that the motor module is prepared\n",
    "    - setting ```motor_prepared``` to ```False``` would signal that we believe the model to be in a situation in which it did not use the motor module that controls key presses in the last few moments\n",
    "    - this would make sense if we tried to model the first item in the experiment\n",
    "    - but the lexical decision tasks are long and repetitive and so, it is more realistic to assume that participants have their motor module in a ready state\n",
    "    - setting ```motor_prepared``` to ```True``` assumes that there is no preparation phase in key presses; otherwise, the module would need 250 ms before executing any manual action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other parameters that can be explicitly set when initializing a model can be listed together with their default values by accessing the ```MODEL_PARAMETERS``` attribute (we will explain the majority of them as we proceed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsymbolic': False,\n",
       " 'rule_firing': 0.05,\n",
       " 'latency_factor': 0.1,\n",
       " 'latency_exponent': 1.0,\n",
       " 'decay': 0.5,\n",
       " 'baselevel_learning': True,\n",
       " 'optimized_learning': False,\n",
       " 'instantaneous_noise': 0,\n",
       " 'retrieval_threshold': 0,\n",
       " 'buffer_spreading_activation': {},\n",
       " 'spreading_activation_restricted': False,\n",
       " 'strength_of_association': 0,\n",
       " 'association_only_from_chunks': True,\n",
       " 'partial_matching': False,\n",
       " 'mismatch_penalty': 1,\n",
       " 'activation_trace': False,\n",
       " 'utility_noise': 0,\n",
       " 'utility_learning': False,\n",
       " 'utility_alpha': 0.2,\n",
       " 'motor_prepared': False,\n",
       " 'strict_harvesting': False,\n",
       " 'production_compilation': False,\n",
       " 'automatic_visual_search': True,\n",
       " 'emma': True,\n",
       " 'emma_noise': True,\n",
       " 'emma_landing_site_noise': False,\n",
       " 'eye_mvt_angle_parameter': 1,\n",
       " 'eye_mvt_scaling_parameter': 0.01}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_decision.MODEL_PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add the modules we used before:\n",
    "\n",
    "- since we are simulating a lexical decision task, we will add some words to declarative memory that the model can access and check against the stimuli in the simulated experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actr.chunktype(\"goal\", \"state\")\n",
    "actr.chunktype(\"word\", \"form\")\n",
    "\n",
    "dm = lex_decision.decmem\n",
    "\n",
    "for string in {\"elephant\", \"dog\", \"crocodile\"}:\n",
    "    dm.add(actr.makechunk(typename=\"word\", form=string))\n",
    "\n",
    "g = lex_decision.goal\n",
    "\n",
    "g.add(actr.makechunk(nameofchunk=\"beginning\",\n",
    "                     typename=\"goal\",\n",
    "                     state=\"start\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add three words to our declarative memory using a Python ```for``` loop.\n",
    "\n",
    "- this way of adding chunks to memory can save a lot of time if we want to add a lot of elements, e.g., a reasonably sized lexicon.\n",
    "\n",
    "We also add a chunk into the goal buffer that will get our lexical decision simulation started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The visual module\n",
    "\n",
    "The visual module allows the ACT-R model to 'see' the environment. This interaction happens via two buffers: \n",
    "\n",
    "- ```visual_location``` searches the environment for elements matching its search criteria\n",
    "- ```visual``` stores the element found using ```visual_location```\n",
    "\n",
    "The two buffers are sometimes called the visual _Where_ and _What_ buffers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visual _Where_ buffer searches the environment (the screen) and outputs the location of an element on the screen that matches some search criteria. Visual search cues have three possible slots:\n",
    "\n",
    "- ```color```\n",
    "- ```screen_x```: the horizontal position on the screen\n",
    "- ```screen_y```: the vertical position on the screen)\n",
    "\n",
    "The $x$ and $y$ positions can be specified:\n",
    "\n",
    "- in precise terms, e.g., find an element at location ```screen_x 100 screen_y 100```, where the numbers represent pixels\n",
    "- approximately:\n",
    "    - a ```screen_x <100``` cue would search for elements at screen locations at most 100 pixels from the left edge of the screen\n",
    "    - a ```screen_x >100``` cue would search for elements on the complementary side of the screen.\n",
    "\n",
    "Three other values are possible for the ```screen_x``` and ```screen_y``` slots:\n",
    "\n",
    "- ```screen_x lowest``` searches for the element with the lowest position on the horizontal axis (the element closest to the left edge);\n",
    "- ```screen_x highest``` searches for the element with the highest position on the same axis (the element closest to the right edge);\n",
    "- ```screen_x closest``` searches for the closest element to the current focus position (the axis is actually ignored in this case).\n",
    "\n",
    "The same applies to the ```screen_y``` slot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visual _What_ buffer stores the element whose location was identified by the _Where_ buffer.\n",
    "\n",
    "- the _What_ buffer is therefore accessed after the _Where_ buffer, as we will see when we state the production rules for our lexical decision model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vision module as a whole is an implementation of an EMMA (Eye Movements and Movement of Attention) model (Salvucci, Dario D. 2001. An integrated model of eye movements and visual encoding. _Cognitive Systems Research_ 1:201–220), which in turn is a generalization and simplification of the E-Z Reader model (Reichle, Erik D, Alexander Pollatsek, Donald L Fisher, and Keith Rayner. 1998. Toward a model of eye movement control in reading. _Psychological Review_ 105:125).\n",
    "\n",
    "- while the latter model is used for reading, the EMMA model attempts to simulate any visual task, not just reading\n",
    "\n",
    "See Staub, Adrian. 2011. Word recognition and syntactic attachment in reading: Evidence for a staged architecture. _Journal of Experimental Psychology: General_ 140:407–433 for a more recent discussion of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The motor module\n",
    "\n",
    "The motor module is limited to the simulation of a key press on the keyboard -- or typing, if multiple key strokes are chained.\n",
    "\n",
    "The ACT-R typing model is based on EPIC's Manual Motor Processor (Meyer, David E, and David E Kieras. 1997. A computational theory of executive cognitive processes and multiple-task performance: Part I. Basic mechanisms. _Psychological Review_ 104:3).\n",
    "\n",
    "It has one buffer that accepts requests to execute motor commands.\n",
    "\n",
    "- the ACT-R motor module currently implemented in ```pyactr``` is more limited, it currently supports only one command: ```press_key```\n",
    "- this should suffice for simulations of many experimental tasks used in (psycho)linguistics, including lexical decision tasks, self-paced reading, forced-choice tasks etc.\n",
    "- all of these tasks commonly require only basic keyboard interaction (or mouse button presses, which we will subsume under keyboard interaction) on the participants' part\n",
    "\n",
    "The hands of the ACT-R model are assumed to be positioned in the home row position on a standard (US) English keyboard, with index fingers at F and J.\n",
    "\n",
    "- the model assumes a competent, albeit not expert, typist.\n",
    "\n",
    "![Hands in the home row position](./figures/hands_home_row_modified.jpg)\n",
    "\n",
    "(source:[Wikipedia](https://upload.wikimedia.org/wikipedia/commons/0/0d/QWERTY-home-keys-position.svg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
