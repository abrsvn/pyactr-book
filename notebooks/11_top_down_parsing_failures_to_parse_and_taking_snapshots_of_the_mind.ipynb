{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](./images/colab-badge.png)](https://colab.research.google.com/github/abrsvn/pyactr-book/blob/master/notebooks/11_top_down_parsing_failures_to_parse_and_taking_snapshots_of_the_mind.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model up to this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyactr as actr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actr.chunktype(\"parsing_goal\", \"stack_top stack_bottom parsed_word task\")\n",
    "actr.chunktype(\"sentence\", \"word1 word2 word3\")\n",
    "actr.chunktype(\"word\", \"form, cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = actr.ACTRModel()\n",
    "dm = parser.decmem\n",
    "g = parser.goal\n",
    "imaginal = parser.set_goal(name=\"imaginal\", delay=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add(actr.chunkstring(string=\"\"\"\n",
    "    isa parsing_goal\n",
    "    task parsing\n",
    "    stack_top S\n",
    "\"\"\"))\n",
    "\n",
    "imaginal.add(actr.chunkstring(string=\"\"\"\n",
    "    isa sentence\n",
    "    word1 Mary\n",
    "    word2 likes\n",
    "    word3 Bill\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.add(actr.chunkstring(string=\"\"\"\n",
    "    isa word\n",
    "    form Mary\n",
    "    cat ProperN\n",
    "\"\"\"))\n",
    "dm.add(actr.chunkstring(string=\"\"\"\n",
    "    isa word\n",
    "    form Bill\n",
    "    cat ProperN\n",
    "\"\"\"))\n",
    "dm.add(actr.chunkstring(string=\"\"\"\n",
    "    isa word\n",
    "    form likes\n",
    "    cat V\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.productionstring(name=\"expand: S ==> NP VP\", string=\"\"\"\n",
    "    =g>\n",
    "    isa parsing_goal\n",
    "    task parsing\n",
    "    stack_top S\n",
    "    ==>\n",
    "    =g>\n",
    "    isa parsing_goal\n",
    "    stack_top NP\n",
    "    stack_bottom VP\n",
    "\"\"\")\n",
    "\n",
    "parser.productionstring(name=\"expand: NP ==> ProperN\", string=\"\"\"\n",
    "    =g>\n",
    "    isa parsing_goal\n",
    "    task parsing\n",
    "    stack_top NP\n",
    "    ==>\n",
    "    =g>\n",
    "    isa parsing_goal\n",
    "    stack_top ProperN\n",
    "\"\"\")\n",
    "\n",
    "parser.productionstring(name=\"expand: VP ==> V NP\", string=\"\"\"\n",
    "    =g>\n",
    "    isa parsing_goal\n",
    "    task parsing\n",
    "    stack_top VP\n",
    "    ==>\n",
    "    =g>\n",
    "    isa parsing_goal\n",
    "    stack_top V\n",
    "    stack_bottom NP\n",
    "\"\"\")\n",
    "\n",
    "parser.productionstring(name=\"retrieve: ProperN\", string=\"\"\"\n",
    "    =g>\n",
    "    isa parsing_goal\n",
    "    task parsing\n",
    "    stack_top ProperN\n",
    "    =imaginal>\n",
    "    isa sentence\n",
    "    word1 =w1\n",
    "    ==>\n",
    "    =g>\n",
    "    isa parsing_goal\n",
    "    task retrieving\n",
    "    +retrieval>\n",
    "    isa word\n",
    "    form =w1\n",
    "\"\"\")\n",
    "\n",
    "parser.productionstring(name=\"retrieve: V\", string=\"\"\"\n",
    "    =g>\n",
    "    isa parsing_goal\n",
    "    task parsing\n",
    "    stack_top V\n",
    "    =imaginal>\n",
    "    isa sentence\n",
    "    word1 =w1\n",
    "    ==>\n",
    "    =g>\n",
    "    isa parsing_goal\n",
    "    task retrieving\n",
    "    +retrieval>\n",
    "    isa word\n",
    "    form =w1\n",
    "\"\"\")\n",
    "\n",
    "parser.productionstring(name=\"scan: word\", string=\"\"\"\n",
    "    =g>\n",
    "    isa parsing_goal\n",
    "    task retrieving\n",
    "    stack_top =y\n",
    "    stack_bottom =x\n",
    "    =retrieval>\n",
    "    isa word\n",
    "    form =w1\n",
    "    cat =y\n",
    "    =imaginal>\n",
    "    isa sentence\n",
    "    word1 =w1\n",
    "    word2 =w2\n",
    "    word3 =w3\n",
    "    ==>\n",
    "    =g>\n",
    "    isa parsing_goal\n",
    "    task printing\n",
    "    stack_top =x\n",
    "    stack_bottom None\n",
    "    parsed_word =w1\n",
    "    =imaginal>\n",
    "    isa sentence\n",
    "    word1 =w2\n",
    "    word2 =w3\n",
    "    word3 None\n",
    "    ~retrieval>\n",
    "\"\"\")\n",
    "\n",
    "parser.productionstring(name=\"print parsed word\", string=\"\"\"\n",
    "    =g>\n",
    "    isa parsing_goal\n",
    "    task printing\n",
    "    =imaginal>\n",
    "    isa sentence\n",
    "    word1 ~None\n",
    "    ==>\n",
    "    !g>\n",
    "    show parsed_word\n",
    "    =g>\n",
    "    isa parsing_goal\n",
    "    task parsing\n",
    "    parsed_word None\n",
    "\"\"\")\n",
    "\n",
    "parser.productionstring(name=\"done\", string=\"\"\"\n",
    "    =g>\n",
    "    isa parsing_goal\n",
    "    task printing\n",
    "    =imaginal>\n",
    "    isa sentence\n",
    "    word1 None\n",
    "    ==>\n",
    "    !g>\n",
    "    show parsed_word\n",
    "    ~imaginal>\n",
    "    ~g>\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser_sim = parser.simulation()\n",
    "# parser_sim.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failures to parse and taking snapshots of the mind when it fails\n",
    "\n",
    "We can run the parser on ungrammatical sentences to see if and how exactly it fails.\n",
    "\n",
    "Let's try to parse the word sequence _Bill Mary likes_.\n",
    "- the parser should fail while parsing the second word _Mary_ because the noun does not match its expectation to see a verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the relevant chunks to the goal and imaginal buffers and start a new simulation.\n",
    "\n",
    "- in general, you should reset the declarative memory module (and various buffers) before rerunning a model simulation\n",
    "- a simple way to reset the model is to reinitialize it from scratch, that is, restart with ```parser = actr.ACTRModel()``` etc.\n",
    "\n",
    "    - you can take a look at the code for the more advanced models of lexical decision tasks to see how to reset the state of a model (without restarting it from scratch) so that multiple simulations with the same initial position can be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add(actr.chunkstring(string=\"\"\"\n",
    "    isa parsing_goal\n",
    "    task parsing\n",
    "    stack_top S\n",
    "\"\"\"))\n",
    "imaginal.add(actr.chunkstring(string=\"\"\"\n",
    "    isa sentence\n",
    "    word1 Bill\n",
    "    word2 Mary\n",
    "    word3 likes\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0, 'PROCEDURAL', 'RULE SELECTED: expand: S ==> NP VP')\n",
      "(0.05, 'PROCEDURAL', 'RULE FIRED: expand: S ==> NP VP')\n",
      "(0.05, 'g', 'MODIFIED')\n",
      "(0.05, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.05, 'PROCEDURAL', 'RULE SELECTED: expand: NP ==> ProperN')\n",
      "(0.1, 'PROCEDURAL', 'RULE FIRED: expand: NP ==> ProperN')\n",
      "(0.1, 'g', 'MODIFIED')\n",
      "(0.1, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.1, 'PROCEDURAL', 'RULE SELECTED: retrieve: ProperN')\n",
      "(0.15, 'PROCEDURAL', 'RULE FIRED: retrieve: ProperN')\n",
      "(0.15, 'g', 'MODIFIED')\n",
      "(0.15, 'retrieval', 'START RETRIEVAL')\n",
      "(0.15, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.15, 'PROCEDURAL', 'NO RULE FOUND')\n",
      "(0.2, 'retrieval', 'CLEARED')\n",
      "(0.2, 'retrieval', 'RETRIEVED: word(cat= ProperN, form= Bill)')\n",
      "(0.2, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.2, 'PROCEDURAL', 'RULE SELECTED: scan: word')\n",
      "(0.25, 'PROCEDURAL', 'RULE FIRED: scan: word')\n",
      "(0.25, 'g', 'MODIFIED')\n",
      "(0.25, 'imaginal', 'MODIFIED')\n",
      "(0.25, 'retrieval', 'CLEARED')\n",
      "(0.25, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.25, 'PROCEDURAL', 'RULE SELECTED: print parsed word')\n",
      "(0.3, 'PROCEDURAL', 'RULE FIRED: print parsed word')\n",
      "parsed_word Bill\n",
      "(0.3, 'g', 'EXECUTED')\n",
      "(0.3, 'g', 'MODIFIED')\n",
      "(0.3, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.3, 'PROCEDURAL', 'RULE SELECTED: expand: VP ==> V NP')\n",
      "(0.35, 'PROCEDURAL', 'RULE FIRED: expand: VP ==> V NP')\n",
      "(0.35, 'g', 'MODIFIED')\n",
      "(0.35, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.35, 'PROCEDURAL', 'RULE SELECTED: retrieve: V')\n",
      "(0.4, 'PROCEDURAL', 'RULE FIRED: retrieve: V')\n",
      "(0.4, 'g', 'MODIFIED')\n",
      "(0.4, 'retrieval', 'START RETRIEVAL')\n",
      "(0.4, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.4, 'PROCEDURAL', 'NO RULE FOUND')\n",
      "(0.45, 'retrieval', 'CLEARED')\n",
      "(0.45, 'retrieval', 'RETRIEVED: word(cat= ProperN, form= Mary)')\n",
      "(0.45, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.45, 'PROCEDURAL', 'NO RULE FOUND')\n"
     ]
    }
   ],
   "source": [
    "parser_sim2 = parser.simulation()\n",
    "parser_sim2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- just as before, our goal is to parse a sentence ```S``` (line 4), namely _Bill Mary likes_ (lines 8-10)\n",
    "- the parser correctly parses the first word _Bill_ and prints it (line 40)\n",
    "\n",
    "The parsing process stops after $450$ ms because:\n",
    "- the word _Mary_ retrieved from declarative memory is of category ProperN (line 55)\n",
    "- but the top of the parsing goal stack stores the category V, which is what the parser was expecting to retrieve (lines 48-49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate the inspection of simulations and models, ```pyactr``` provides a way to advance simulations one step at a time rather than letting them run from beginning to end.\n",
    "\n",
    "This makes it easy to check the internal state of the buffers, as well as to diagnose / debug our models.\n",
    "- e.g., if the model gets stuck in an infinite loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the simulation again and go through it step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add(actr.chunkstring(string=\"\"\"\n",
    "    isa parsing_goal\n",
    "    task parsing\n",
    "    stack_top S\n",
    "\"\"\"))\n",
    "imaginal.add(actr.chunkstring(string=\"\"\"\n",
    "    isa sentence\n",
    "    word1 Bill\n",
    "    word2 Mary\n",
    "    word3 likes\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n"
     ]
    }
   ],
   "source": [
    "parser_sim3 = parser.simulation()\n",
    "parser_sim3.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very little happens in the first step:\n",
    "- the parser simply enters a 'conflict resolution' state in which it identifies the rules that can be fired given the initial cognitive state (that is, the initial state of the buffers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through some more steps.\n",
    "- to do that, we use the method ```steps``` with a parameter that provides the exact number of steps the simulation should advance through\n",
    "\n",
    "Let's advance $10$ steps, which are reflected in the $10$ lines of simulation output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'PROCEDURAL', 'RULE SELECTED: expand: S ==> NP VP')\n",
      "(0.05, 'PROCEDURAL', 'RULE FIRED: expand: S ==> NP VP')\n",
      "(0.05, 'g', 'MODIFIED')\n",
      "(0.05, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.05, 'PROCEDURAL', 'RULE SELECTED: expand: NP ==> ProperN')\n",
      "(0.1, 'PROCEDURAL', 'RULE FIRED: expand: NP ==> ProperN')\n",
      "(0.1, 'g', 'MODIFIED')\n",
      "(0.1, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.1, 'PROCEDURAL', 'RULE SELECTED: retrieve: ProperN')\n",
      "(0.15, 'PROCEDURAL', 'RULE FIRED: retrieve: ProperN')\n"
     ]
    }
   ],
   "source": [
    "parser_sim3.steps(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's advance our simulation to the point where the rule ```\"scan: word\"``` has just fired. To do that, we have to be able to:\n",
    "\n",
    "- check the current event, i.e., the most recent step taken in the simulation, and\n",
    "- stop when this event is a ```\"scan: word\"```-rule firing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current event is an attribute of the simulation.\n",
    "- for example, the current event in our simulation is a ProperN retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Event(time=0.15, proc='PROCEDURAL', action='RULE FIRED: retrieve: ProperN')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser_sim3.current_event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The event has three attributes:\n",
    "\n",
    "- ```time```: the simulation time at which the event occurred ($150$ ms in our case)\n",
    "- ```proc```: the module that is affected (procedural memory in our case)\n",
    "- ```action```: the cognitive action that has taken place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now advance to the first firing of the ```\"scan: word\"``` rule.\n",
    "\n",
    "- we do this by running a ```while``` loop in the Python interpreter\n",
    "    - the command on line 2 below, i.e., advance one step through the simulation, should be taken while the condition on line 1 is satisfied\n",
    "    - that condition says that the ```action``` attribute of the current event should _not_ be a ```\"scan: word\"``` firing\n",
    "\n",
    "- ```!=``` is non-identity in Python, as we already discussed in the notebook introducing Python\n",
    "- ```!``` is customarily used for negation in programming languages, and it is distinct from ACT-R negation ```~``` that we sometimes use inside production rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.15, 'g', 'MODIFIED')\n",
      "(0.15, 'retrieval', 'START RETRIEVAL')\n",
      "(0.15, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.15, 'PROCEDURAL', 'NO RULE FOUND')\n",
      "(0.2, 'retrieval', 'CLEARED')\n",
      "(0.2, 'retrieval', 'RETRIEVED: word(cat= ProperN, form= Bill)')\n",
      "(0.2, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.2, 'PROCEDURAL', 'RULE SELECTED: scan: word')\n",
      "(0.25, 'PROCEDURAL', 'RULE FIRED: scan: word')\n"
     ]
    }
   ],
   "source": [
    "while parser_sim3.current_event.action != 'RULE FIRED: scan: word':\n",
    "    parser_sim3.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take 3 more steps so that the changes triggered by the ```\"scan: word\"``` rule are recorded in the ACT-R mind / model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.25, 'g', 'MODIFIED')\n",
      "(0.25, 'imaginal', 'MODIFIED')\n",
      "(0.25, 'retrieval', 'CLEARED')\n"
     ]
    }
   ],
   "source": [
    "parser_sim3.steps(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect our buffers at this point in the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{parsing_goal(parsed_word= Bill, stack_bottom= None, stack_top= VP, task= printing)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- as expected, the top of our parsing goal stack is a VP nonterminal\n",
    "    - the ProperN nonterminal has just been popped off the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{sentence(word1= Mary, word2= likes, word3= None)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imaginal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the first word _Bill_ has been removed from the sentence stored in the imaginal buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the lexical representation for _Bill_ has been flushed from the retrieval buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now advance to the point where the parsing process failed.\n",
    "\n",
    "- we will step through the simulation until the ```action``` attribute of the current event starts with the string ```'RETRIEVED'```\n",
    "- that will be the point where the second word in our string, namely _Mary_, has been retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.25, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.25, 'PROCEDURAL', 'RULE SELECTED: print parsed word')\n",
      "(0.3, 'PROCEDURAL', 'RULE FIRED: print parsed word')\n",
      "parsed_word Bill\n",
      "(0.3, 'g', 'EXECUTED')\n",
      "(0.3, 'g', 'MODIFIED')\n",
      "(0.3, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.3, 'PROCEDURAL', 'RULE SELECTED: expand: VP ==> V NP')\n",
      "(0.35, 'PROCEDURAL', 'RULE FIRED: expand: VP ==> V NP')\n",
      "(0.35, 'g', 'MODIFIED')\n",
      "(0.35, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.35, 'PROCEDURAL', 'RULE SELECTED: retrieve: V')\n",
      "(0.4, 'PROCEDURAL', 'RULE FIRED: retrieve: V')\n",
      "(0.4, 'g', 'MODIFIED')\n",
      "(0.4, 'retrieval', 'START RETRIEVAL')\n",
      "(0.4, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.4, 'PROCEDURAL', 'NO RULE FOUND')\n",
      "(0.45, 'retrieval', 'CLEARED')\n",
      "(0.45, 'retrieval', 'RETRIEVED: word(cat= ProperN, form= Mary)')\n"
     ]
    }
   ],
   "source": [
    "while not parser_sim3.current_event.action.startswith('RETRIEVED'):\n",
    "    parser_sim3.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can once again inspect the current cognitive state of the model / mind, i.e., the buffer contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{parsing_goal(parsed_word= None, stack_bottom= NP, stack_top= V, task= retrieving)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{sentence(word1= Mary, word2= likes, word3= None)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imaginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{word(cat= ProperN, form= Mary)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cause of the parsing failure is apparent:\n",
    "\n",
    "- the retrieval buffer stores a ProperN\n",
    "- but the top of the parsing goal stack, i.e., our current parsing expectation / prediction, is a V\n",
    "\n",
    "The parser therefore halts before the second word in our sentence can be scanned, as shown by the unchanged chunk in the imaginal buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-down parsing as an imperfect psycholinguistic model\n",
    "\n",
    "It is not enough for our parser to correctly parse grammatical sentences, and fail to parse ungrammatical ones.\n",
    "\n",
    "- our top-down ACT-R parser is not simply an implementation of an arbitrary parsing algorithm that is satisfactory as long as it works correctly\n",
    "- this parser is meant to be a limited but realistic model of a certain kind of human cognitive behavior:\n",
    "    - syntactic parsing in sentence comprehension tasks (e.g., self-paced reading).\n",
    "    \n",
    "Is our parser even remotely adequate as a psycholinguistic model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the empirical adequacy desiderata for our parser is that the temporal trace of parsing a sentence should correspond to the temporal trace of an average human participant completing the same task.\n",
    "\n",
    "- for example, we see that our parser takes $800$ ms to parse the sentence _Mary likes Bill_\n",
    "- this is roughly correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But other properties of our parser are more worrying:\n",
    "\n",
    "- for one, the parser requires this much time while abstracting away from what human participants have to do during an actual self-paced reading task:\n",
    "    - internalizing visual information\n",
    "    - projecting sentence meaning\n",
    "    - executing motor actions (pressing keys) etc.\n",
    "- so ultimately $800$ ms might be too much given the very narrow amount of work our parser actually does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- another issue is that retrieving lexical information always takes $50$ ms in our current models and simulations\n",
    "    - this is hardly realistic\n",
    "    - we know that lexical retrieval is dependent on various factors, word frequency, priming etc.\n",
    "    - these factors are completely ignored here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- finally, top-down parsers work well for right-branching structures like the sentence _Mary likes Bill_, but they have significant difficulties with left branching structures.\n",
    "    - for such structures, the parser would have to store as many symbols on the stack as there are levels of embedding\n",
    "    - since every expansion of a rule takes $50$ ms, we expect left branching structures with $n$ levels of embedding to take $50*n$ ms\n",
    "    - this is at odds with actual human performance, see:\n",
    "        - Johnson-Laird, Philip N. 1983. _Mental models: Towards a cognitive science of language, inference, and consciousness_. Harvard University Press\n",
    "        - Abney, Steven, and Mark Johnson. 1991. Memory requirements and local ambiguities of parsing strategies. _Journal of Psycholinguistic Research_ 20:233–50\n",
    "        - Resnik, Philip. 1992. Left-corner parsing and psychological plausibility. In _Proceedings of the Fourteenth International Conference on Computational Linguistics_. Nantes, France.\n",
    "\n",
    "- the main reason for this issue is that our parser generates predictions about syntactic structure\n",
    "    - **exclusively based on the grammar**\n",
    "    - **completely ignoring the actual evidence** (the sentence to be parsed) until it reaches a terminal on the leftmost branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, purely top-down parsers consult the evidence (the word string) only after they predict all the way to lexical items:\n",
    "\n",
    "- such pure top-down parsers would place memory retrieval requests based on the terminal at the top of the parsing goal stack\n",
    "- for example, if a ProperN is at the top of the stack, they would retrieve an arbitrary ProperN from declarative memory and only after that, check whether the form of the retrieved ProperN matches the leftmost word to be parsed\n",
    "- if not, a new retrieval request would be placed for a new ProperN in hopes that the form of that new chunk would match the word to be parsed\n",
    "- in the worst case, such a purely top-down parser would retrieve all chunks of category ProperN one at at a time from declarative memory, and finally identify the one whose form matches the current word to be parsed\n",
    "- the temporal trace of such a parser would be very far from the temporal trace of an average human participant completing the same task:\n",
    "    - if the lexicon contains 20 chunks of ProperN category, and a retrieval takes around $50$ ms, it would take a full second to parse the first word in the sentence _Mary likes Bill_ in the worst-case scenario\n",
    "    - and this ignores the time needed to verify that 19 of the retrieved chunks are mismatches, and then the time needed to backtrack and restart the retrieval process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more plausible human parser would **consult the evidence, i.e., the word string to be parsed, earlier and more often** in the parsing process.\n",
    "\n",
    "- our top-down parsing strategy needs to be complemented by a bottom-up parsing strategy\n",
    "- in principle, we could switch from a purely top-down parser to a purely bottom-up parser that is completely driven by the evidence\n",
    "- such a parser would be incremental, but it would not be predictive in the same way that the human parser seems to be\n",
    "- we will therefore not explore purely bottom-up (shift-reduce) parsers and instead move directly to **left-corner parsers**, which combine top-down and bottom-up features\n",
    "    - they can be thought of as predictive top-down parsers with incremental bottom-up filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
