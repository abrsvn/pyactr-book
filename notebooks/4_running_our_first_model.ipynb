{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](./images/colab-badge.png)](https://colab.research.google.com/github/abrsvn/pyactr-book/blob/master/notebooks/4_running_our_first_model.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running our first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from the previous chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the line below to install pyactr\n",
    "# !pip3 install pyactr\n",
    "\n",
    "import pyactr as actr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actr.chunktype(\"word\", \"form, meaning, category, number, synfunction\")\n",
    "actr.chunktype(\"goal_lexeme\", \"task, category, number\")\n",
    "\n",
    "carLexeme = actr.chunkstring(string=\"\"\"\n",
    "    isa word\n",
    "    form car\n",
    "    meaning '[[car]]'\n",
    "    category noun\n",
    "    number sg\n",
    "    synfunction subject\n",
    "\"\"\")\n",
    "\n",
    "agreement = actr.ACTRModel()\n",
    "dm = agreement.decmem\n",
    "dm.add(carLexeme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'=g': goal_lexeme(category= , number= , task= done)}\n",
       "==>\n",
       "{'~g': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreement.productionstring(name=\"retrieve\", string=\"\"\"\n",
    "    =g>\n",
    "    isa goal_lexeme\n",
    "    category verb\n",
    "    task agree\n",
    "    ?retrieval>\n",
    "    buffer empty\n",
    "    ==>\n",
    "    =g>\n",
    "    isa goal_lexeme\n",
    "    task trigger_agreement\n",
    "    category verb\n",
    "    +retrieval>\n",
    "    isa word\n",
    "    category noun\n",
    "    synfunction subject\n",
    "\"\"\")\n",
    "\n",
    "agreement.productionstring(name=\"agree\", string=\"\"\"\n",
    "    =g>\n",
    "    isa goal_lexeme\n",
    "    task trigger_agreement\n",
    "    category verb\n",
    "    =retrieval>\n",
    "    isa word\n",
    "    category noun\n",
    "    synfunction subject\n",
    "    number =x\n",
    "    ==>\n",
    "    =g>\n",
    "    isa goal_lexeme\n",
    "    category verb\n",
    "    number =x\n",
    "    task done\n",
    "\"\"\")\n",
    "\n",
    "agreement.productionstring(name=\"done\", string=\"\"\"\n",
    "    =g>\n",
    "    isa goal_lexeme\n",
    "    task done\n",
    "    ==>\n",
    "    ~g>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the agreement model, we just have to add an appropriate chunk to the goal buffer. Recall that ACT-R conceptualizes higher cognition as fundamentally goal-driven: if there is no goal, no productions will fire and the mind will not change state.\n",
    "\n",
    "We add a goal chunk below. First, we declare our ```goal_lexeme``` type (line 1). Then, we add one such chunk to the goal buffer (lines 2-6). Chunks are always added to buffers / modules using the method ```add```. We check that the chunk has been added to the goal buffer by printing its contents (line 7). Note that the number specification on line 8 is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{goal_lexeme(category= verb, number= , task= agree)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreement.goal.add(actr.chunkstring(string=\"\"\"\n",
    "    isa goal_lexeme\n",
    "    task agree\n",
    "    category verb\n",
    "    \"\"\"))\n",
    "agreement.goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the model by invoking the ```simulation``` method (with no arguments), as shown in line 1 below. This takes the model specification and initializes various parameters as dictated by the model (e.g., simulation start time). We can then execute one run of the simulation, as shown on line 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0, 'PROCEDURAL', 'RULE SELECTED: retrieve')\n",
      "(0.05, 'PROCEDURAL', 'RULE FIRED: retrieve')\n",
      "(0.05, 'g', 'MODIFIED')\n",
      "(0.05, 'retrieval', 'START RETRIEVAL')\n",
      "(0.05, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.05, 'PROCEDURAL', 'NO RULE FOUND')\n",
      "(0.1, 'retrieval', 'CLEARED')\n",
      "(0.1, 'retrieval', 'RETRIEVED: word(category= noun, form= car, meaning= [[car]], number= sg, synfunction= subject)')\n",
      "(0.1, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.1, 'PROCEDURAL', 'RULE SELECTED: agree')\n",
      "(0.15, 'PROCEDURAL', 'RULE FIRED: agree')\n",
      "(0.15, 'g', 'MODIFIED')\n",
      "(0.15, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.15, 'PROCEDURAL', 'RULE SELECTED: done')\n",
      "(0.2, 'PROCEDURAL', 'RULE FIRED: done')\n",
      "(0.2, 'g', 'CLEARED')\n",
      "(0.2, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.2, 'PROCEDURAL', 'NO RULE FOUND')\n"
     ]
    }
   ],
   "source": [
    "agreement_sim = agreement.simulation()\n",
    "agreement_sim.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the ```run()``` command is the temporal trace of our model simulation. Each line specifies three elements:\n",
    "\n",
    "1. the simulation time (in seconds);\n",
    "2. the module (name in upper-case letters) or buffer (name in lower-case letters) that is affected;\n",
    "3. a description of what is happening to the module or buffer. By default, every cognitive step in the model takes $50$ ms, i.e., $0.05$ seconds; this is the ACT-R default time for an elementary cognitive operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line of our temporal trace states that conflict resolution is taking place in the procedural memory module, i.e., the module where all the production rules reside. This happens at simulation time $0$. The main function of 'conflict resolution' is to examine the current state of the mind (basically, the state of the buffers in our model) and to determine if any production rule can apply, i.e., to check if the current state of the mind satisfies the preconditions of any production rule.\n",
    "\n",
    "Note how ACT-R once again combines serial and parallel components to capture actual cognitive behavior. Checking if the current state of the mind satisfies the preconditions of any rule is a massively parallel process: all rules are simultaneously and very quickly (instantaneously) checked. But rule firing is serial: at any given point in the cognitive process, only one rule can fire / apply. This is similar to the interaction between the parallel computations in the declarative memory module (all chunks are simultaneously checked against a pattern / cue) and the serial way in which retrieval cues can be placed in the retrieval buffer (one at a time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Conflict resolution' is particularly simple in the present case. Given the state of the goal and retrieval buffers, only one rule can apply: our first production rule, which we named ```retrieve``` above. Line 4 in our temporal trace shows that the ```retrieve``` rule is selected at time $0$. The rule fires, and this takes the ACT-R default time of $50$ ms, as shown on line 5. The state of our mind has changed as a consequence of this rule firing, and the subsequent lines in the output report on that new state: the goal buffer has been modified (line 6; the task is now ```trigger_agreement```) and the retrieval buffer has started a memory retrieval procedure (line 7), which will take time to complete.\n",
    "\n",
    "Now that the ```retrieve``` rule has fired, the procedural module enters a 'conflict resolution' state again and looks for production rules to apply (line 8). The current state of the mind (i.e., the buffer state) does not satisfy the preconditions of any rule, so none is fired (line 9).\n",
    "\n",
    "However, a memory retrieval process has been started and is completed $50$ ms later, i.e., at the next simulation time of $100$ ms. Retrieval time is set to a default value of $50$ ms here, but ACT-R specifies in great detail how memory behaves, and makes clear predictions about retrieval accuracy and retrieval latency. This is discussed in detail in Chapter 6, but we want to keep our first model simple so we use the default retrieval time of $50$ ms here.\n",
    "\n",
    "At the $100$ ms point, the memory retrieval process has been completed and the retrieval buffer is cleared (line 10) so that the newly retrieved chunk can be placed there (lines 11-12).\n",
    "\n",
    "The mind is now in a new state since the buffer contents have changed, so the procedural module reenters a `conflict resolution' state of rule collection \\& rule selection (line 13). This time, the resolution process identifies one rule that can fire (line 14), namely the second production rule we discussed above and which we named ```agree```.\n",
    "\n",
    "The ```agree``` rule takes $50$ ms to fire (line 15), so we are now at $150$ ms in simulation time. As a consequence of the ```agree``` rule, the chunk in the goal buffer has been modified (line 16): its number specification has been updated so that it is now the same number as the noun chunk in the retrieval buffer.\n",
    "\n",
    "Agreement has been performed, so the third and final production rule is selected (lines 17-18). The rule takes $50$ ms to fire (line 19), so at time $0.2$ s, the goal buffer is cleared (line 20), and no further rule can apply (lines 21-22).\n",
    "\n",
    "When the goal buffer is cleared, the information stored in it does not disappear. The ACT-R architecture specifies that the cleared information is automatically transferred (`harvested') to declarative memory. The intuition behind this is that our past accomplished goals, i.e., the results of our past successful cognitive processes, become our present (newly acquired) memory facts. This is also the case in ```pyactr```. We can inspect the final state of the declarative memory module to see that it stores the cleared goal-buffer chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{word(category= noun, form= car, meaning= [[car]], number= sg, synfunction= subject): array([0.]), goal_lexeme(category= verb, number= sg, task= done): array([0.2])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this newly added chunk is time-stamped with the simulation time at which the goal buffer was cleared ($0.2$ s).\n",
    "\n",
    "And that's it. At its core, ACT-R provides a fairly simple framework for building process models that is accessible to generative linguists because it is production-rule based and manipulates feature structures of a familiar kind.\n",
    "\n",
    "To be sure, our first model and the introduction to ACT-R and ```pyactr``` in this chapter are overly simplistic in many ways. But the main point is that we can now start building explicit and more realistic computational models for linguistic processes and behaviors.\n",
    "\n",
    "Our development of integrated competence-performance theories for linguistic phenomena is now at a stage similar to the one in a formal semantics intro course where the semantics for classical first order logic (FOL) has just been introduced. FOL semantics is in many ways an overly simplistic model for natural language semantics, but it provides the basic structure that more realistic theories of natural language interpretation (in the Montagovian tradition) can build on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
