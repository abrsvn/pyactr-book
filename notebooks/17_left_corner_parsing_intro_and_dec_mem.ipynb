{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](./images/colab-badge.png)](https://colab.research.google.com/github/abrsvn/pyactr-book/blob/master/notebooks/17_left_corner_parsing_intro_and_dec_mem.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A left-corner parser with visual and motor interfaces\n",
    "\n",
    "In this notebook, we introduce a left-corner parser that incorporates visual and motor interfaces. The left-corner parser builds on:\n",
    "- the basic top-down parser we introduced before, and\n",
    "- the lexical decision model with visual and motor interfaces we just discussed.\n",
    "\n",
    "As we mentioned before, left-corner parsers combine top-down and bottom-up features: they can be thought of as predictive top-down parsers with incremental bottom-up filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left-corner parsing differs from top-down parsing with respect to the amount of evidence necessary to trigger a production rule:\n",
    "\n",
    "- a grammar rule cannot be triggered without any evidence from the incoming signal / string of words, as it would be in a top-down parser\n",
    "- but we do not need to accumulate complete evidence, that is, all the necessary words, to trigger a rule, as we would in bottom-up parsing\n",
    "    - for example, we do not need both words in _Mary sleeps_ to trigger the ```S -> NP VP``` rule\n",
    "\n",
    "Thus, in left-corner parsing, partial evidence is:\n",
    "\n",
    "- necessary, in contrast to top-down parsing),\n",
    "- and also sufficient, in contrast to bottom-up parsing\n",
    "\n",
    "For example, having evidence for the very first category on the right-hand side of the rule, namely\n",
    "\n",
    "- the NP in the sentence _Mary sleeps_\n",
    "- which we describe as having evidence for the _left corner_ of the ```S -> NP VP``` rule\n",
    "\n",
    "is sufficient to trigger it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Hale, John T. 2014. _Automaton theories of human sentence comprehension_. Stanford: CSLI Publications, we summarize the left-corner parsing strategy in the 'project' and 'project & complete' rules below.\n",
    "\n",
    "- the only difference between them is the context in which the left-corner rule is triggered:\n",
    "    - if the mother node, e.g., ```S``` in our example above, is not expected in context, it is added to the context as a 'found' symbol\n",
    "        - this is the simple 'project' rule\n",
    "     - but if the mother node is already expected in context, we check off that expectation as satisfied\n",
    "        - this is the 'project & complete' rule \n",
    "\n",
    "Finally, the 'shift' rule takes words one at a time from the incoming string of words and adds them to the top of the stack to be parsed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Left-corner parsing rule schemata** (Hale, John T. 2014. _Automaton theories of human sentence comprehension_. Stanford: CSLI Publications):\n",
    "\n",
    "- __Project__:\n",
    "    - if the symbol Y is at the top of the stack, and there is a grammar rule X $\\rightarrow$ Y Z whose right-hand side starts with Y\n",
    "    - then replace Y with two new symbols:\n",
    "        - a record that X has been found, and\n",
    "        - an expectation for the remaining right-hand side symbol(s) Z\n",
    "        \n",
    "- __Project \\& complete__:\n",
    "    - if the symbol Y is at the top of the stack and right below it is an expectation of finding symbol X, and there is a grammar rule X $\\rightarrow$ Y Z\n",
    "    - then replace both Y and X with an expectation for the remaining right-hand side symbol(s) Z\n",
    "\n",
    "- __Shift__: if the next word of the sentence is a terminal symbol of the grammar, push it on the top of the stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distinction between the two different kinds of left-corner projection\n",
    "\n",
    "- projection _tout court_, and\n",
    "- projection plus a completion step\n",
    "\n",
    "was proposed in Resnik, Philip. 1992. Left-corner parsing and psychological plausibility. In _Proceedings of the Fourteenth International Conference on Computational Linguistics_. Nantes, France.\n",
    "\n",
    "Resnik argues that projection & completion is necessary to keep the stack depth reasonably low when parsing both left-branching and right-branching structures.\n",
    "\n",
    "- most of our rules will be project & complete rules, with the exception of NPs projected by ProperNs\n",
    "- if the ProperN is in subject position, it will trigger a simple projection rule for the NP dominating it since we do not have an NP expectation at that point\n",
    "- but if the ProperN is in object position, the previous application of the ```VP -> V NP``` rule added an NP expectation to the context, so we can both project and complete the NP at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a left-corner parser in ACT-R.\n",
    "\n",
    "We start by importing ```pyactr``` and setting the position on the virtual screen where the words in our example -- the simple sentence _Mary likes Bill_ -- will be displayed one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyactr as actr\n",
    "\n",
    "environment = actr.Environment(focus_position=(320, 180))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then declare the chunk types we need:\n",
    "\n",
    "- ```parsing_goal``` chunks will be stored in the goal buffer and they will drive the parsing cognitive process\n",
    "- ```parse_state``` chunks will be stored in the imaginal buffer, and they will provide intermediate internal snapshots of the parsing process, as is befitting of information stored in the imaginal buffer\n",
    "- ```word``` chunks will be stored in declarative memory and encode lexical information (in our case, just phonological form and syntactic category) for the words in our target example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actr.chunktype(\"parsing_goal\", \"task stack_top stack_bottom\\\n",
    "                                parsed_word right_frontier\")\n",
    "actr.chunktype(\"parse_state\", \"node_cat mother daughter1\\\n",
    "                               daughter2 lex_head\")\n",
    "actr.chunktype(\"word\", \"form cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```parsing_goal``` chunk type has the same slots as in the top-down parser discussed before, with the addition of a ```right_frontier``` slot.\n",
    "\n",
    "- the right-frontier slot will be used to record the attachment points for NPs:\n",
    "    - the ```S``` node for subject NPs\n",
    "    - the ```VP``` node for object NPs\n",
    "    \n",
    "Whenever we store a ```parse_state``` chunk in the imaginal buffer that will contain information about an NP that has just been parsed, we will take the value in the ```right_frontier``` slot and record it as the value of the ```mother``` node for the NP in the imaginal buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which brings us to the ```parse_state``` chunk type: these intermediate parsing states that are stored in the imaginal buffer record the progress of the parsing cognitive process.\n",
    "\n",
    "- the ```node_cat``` slot records the syntactic category of the current node, i.e., the node that has just been parsed\n",
    "- the ```mother``` slot records the mother node of the current node\n",
    "- the ```daughter1``` and ```daughter2``` slots record the daughter nodes of the current node\n",
    "- and finally, the ```lex_head``` slot records the lexical head of the current phrasal projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```parse_state``` chunk type gives us a window into how much ACT-R constrains theories of 'high-level' cognitive processes.\n",
    "\n",
    "- the goal of the parsing cognitive process can be characterized as incrementally building an unobservable hierarchical tree structure (a structural description) for the target sentence\n",
    "- but there are strict limits on how the partially-built structure is maintained and accessed during the cognitive process: *we can only store one chunk at a time in any given buffer (goal, imaginal, retrieval)*\n",
    "\n",
    "This means that **the mind never has a global view of the syntactic tree it is constructing**.\n",
    "- instead, the structure is viewed through a limited, moving window that can 'see' only a part of the under-construction structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the values stored in the slots of a chunk can encode only 'descriptive' content, not specific memory addresses (e.g., uniquely identifiable time stamps) of specific nodes in the tree.\n",
    "\n",
    "- this is particularly constraining for phrases like NPs, which are multiply instantiated in a given structure\n",
    "- their position in the hierarchical structure can be identified only if we encode additional information in their corresponding chunks\n",
    "- we need to record the lexical head associated with an NP to be able to identify which word it dominates, otherwise the NP might end up dominating any ProperN that has already been built / parsed\n",
    "    - hence the need for the ```lex_head``` slot\n",
    "- we also need to record the point where the full NP is attached in the larger tree, otherwise we might end up attaching the direct object NP to the S node as if it were a subject\n",
    "    - hence the need for the ```mother``` slot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two slots of the ```parse_state``` chunk type, namely ```lex_head``` and ```mother```, will be exclusively needed for NPs in the left-corner parser introduced in this section.\n",
    "\n",
    "- there is no deep reason for this\n",
    "- for simplicity, we only focus on simple mono-clausal target sentences, so only NP and ProperN nodes will be multiply instantiated in any given tree\n",
    "- when we scale up the parser to include multi-clausal sentences and/or multi-sentential discourses, we will end up using these slots for other node types, e.g., VP and S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialize the ```parser``` model and set up separate variables for the declarative memory module (```dm```), the goal buffer (```g```) and the imaginal buffer (```imaginal```).\n",
    "\n",
    "- we set a delay of $0$ ms for the imaginal buffer, going against its default setting of $200$ ms\n",
    "- this default setting is motivated by non-linguistic cognitive processes that are structurally much simpler than language comprehension\n",
    "    - the $200$ ms encoding delay provides a better fit to the reaction time data associated with those processes\n",
    "- in contrast, a low-delay, or even a no-delay, setting is necessary when modeling language comprehension in ACT-R because this requires rapidly building complex hierarchical representations that are likely to extensively rely on imaginal chunks\n",
    "- in general, it is reasonable to expect that the systematic modeling of language processing in ACT-R -- still very much a nascent endeavor -- will occasionally require such departures from received ACT-R wisdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = actr.ACTRModel(environment, motor_prepared=True)\n",
    "\n",
    "dm = parser.decmem\n",
    "g = parser.goal\n",
    "imaginal = parser.set_goal(name=\"imaginal\", delay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to add lexical entries to declarative memory.\n",
    "\n",
    "- just as in the case of our top-down parser, we keep the lexical information to a minimum and store only phonological forms and syntactic categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.add(actr.chunkstring(string=\"\"\"\n",
    "    isa  word\n",
    "    form Mary\n",
    "    cat  ProperN\n",
    "\"\"\"))\n",
    "dm.add(actr.chunkstring(string=\"\"\"\n",
    "    isa  word\n",
    "    form Bill\n",
    "    cat  ProperN\n",
    "\"\"\"))\n",
    "dm.add(actr.chunkstring(string=\"\"\"\n",
    "    isa  word\n",
    "    form likes\n",
    "    cat  V\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add the starting goal chunk:\n",
    "\n",
    "- the goal is to read the first word and try to parse a sentence (```stack_top``` is ```S```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add(actr.chunkstring(string=\"\"\"\n",
    "    isa             parsing_goal\n",
    "    task            read_word\n",
    "    stack_top       S\n",
    "    right_frontier  S\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
