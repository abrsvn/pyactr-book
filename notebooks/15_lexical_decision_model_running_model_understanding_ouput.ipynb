{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](./images/colab-badge.png)](https://colab.research.google.com/github/abrsvn/pyactr-book/blob/master/notebooks/15_lexical_decision_model_running_model_understanding_ouput.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model up to this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyactr as actr\n",
    "\n",
    "environment = actr.Environment(focus_position=(0,0))\n",
    "lex_decision = actr.ACTRModel(\n",
    "    environment=environment,\n",
    "    automatic_visual_search=False,\n",
    "    motor_prepared=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actr.chunktype(\"goal\", \"state\")\n",
    "actr.chunktype(\"word\", \"form\")\n",
    "\n",
    "dm = lex_decision.decmem\n",
    "\n",
    "for string in {\"elephant\", \"dog\", \"crocodile\"}:\n",
    "    dm.add(actr.makechunk(typename=\"word\", form=string))\n",
    "\n",
    "g = lex_decision.goal\n",
    "\n",
    "g.add(actr.makechunk(nameofchunk=\"beginning\",\n",
    "                     typename=\"goal\",\n",
    "                     state=\"start\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_decision.productionstring(name=\"find word\", string=\"\"\"\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   start\n",
    "    ?visual_location>\n",
    "    buffer  empty\n",
    "    ==>\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   attend\n",
    "    +visual_location>\n",
    "    isa _visuallocation\n",
    "    screen_x closest\n",
    "\"\"\")\n",
    "\n",
    "lex_decision.productionstring(name=\"attend word\", string=\"\"\"\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   attend\n",
    "    =visual_location>\n",
    "    isa    _visuallocation\n",
    "    ?visual>\n",
    "    state   free\n",
    "    ==>\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   retrieving\n",
    "    +visual>\n",
    "    isa     _visual\n",
    "    cmd     move_attention\n",
    "    screen_pos =visual_location\n",
    "    ~visual_location>\n",
    "\"\"\")\n",
    "\n",
    "lex_decision.productionstring(name=\"retrieving\", string=\"\"\"\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   retrieving\n",
    "    =visual>\n",
    "    isa     _visual\n",
    "    value   =val\n",
    "    ==>\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   retrieval_done\n",
    "    +retrieval>\n",
    "    isa     word\n",
    "    form    =val\n",
    "\"\"\")\n",
    "\n",
    "lex_decision.productionstring(name=\"lexeme retrieved\", string=\"\"\"\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   retrieval_done\n",
    "    ?retrieval>\n",
    "    buffer  full\n",
    "    state   free\n",
    "    ==>\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   done\n",
    "    +manual>\n",
    "    isa     _manual\n",
    "    cmd     press_key\n",
    "    key     J\n",
    "\"\"\")\n",
    "\n",
    "lex_decision.productionstring(name=\"no lexeme found\", string=\"\"\"\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   retrieval_done\n",
    "    ?retrieval>\n",
    "    buffer  empty\n",
    "    state   error\n",
    "    ==>\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   done\n",
    "    +manual>\n",
    "    isa     _manual\n",
    "    cmd     press_key\n",
    "    key     F\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the lexical decision model and understanding the output\n",
    "\n",
    "Before we run the simulation of the model, we have to specify the set of stimuli (character strings) that should appear on the screen.\n",
    "\n",
    "- we use a dictionary for that\n",
    "\n",
    "Below, we specify that our first -- and only -- stimulus is the word \\textit{elephant}, which should be displayed on the screen starting at pixel $\\langle 320, 180\\rangle$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = {1: {'text': 'elephant', 'position': (320, 180)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to initialize the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_dec_sim = lex_decision.simulation(\n",
    "    realtime=False,\n",
    "    gui=False,\n",
    "    environment_process=environment.environment_process,\n",
    "    stimuli=word,\n",
    "    triggers='',\n",
    "    times=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the first parameter, namely ```realtime``` (line 2 above), states that the simulation should not appear in real time\n",
    "    - setting this parameter to ```True``` ensures that the simulation will take the same amount of real time as the model predicts\n",
    "    - otherwise, the simulation is executed as fast as the processing speed of the computer allows it\n",
    "    - this does not affect the actual model and its predictions in any way, it only affects the way the simulation is played out\n",
    "    \n",
    "- the second parameter ```gui``` (line 3) specifies whether a graphical user interface should be started in a separate window to represent the environment, i.e., the virtual screen on which the stimuli are displayed\n",
    "    - this option is switched off here, but feel free to switch it on by setting ```gui``` to ```True```   \n",
    "\n",
    "- the third argument (line 4) states what environment process should appear in our environment\n",
    "    - you can create your own, but there is one predefined in the ```Environment``` class that displays stimuli from the list above one at a time on the virtual screen\n",
    "    \n",
    "- the stimulus list to be displayed in the environment is specified by the fourth parameter (line 5)\n",
    "\n",
    "- the penultimate parameter is ```triggers``` (line 6), which specifies the triggers that the process should respond to\n",
    "    - we do not care about any triggers here, so we leave that list empty\n",
    "    \n",
    "- the final parameter is ```times``` (line 7), which specifies that each stimulus should be displayed for $1$ s max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation can now be run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0, 'PROCEDURAL', 'RULE SELECTED: find word')\n",
      "****Environment: {1: {'text': 'elephant', 'position': (320, 180)}}\n",
      "(0.05, 'PROCEDURAL', 'RULE FIRED: find word')\n",
      "(0.05, 'g', 'MODIFIED')\n",
      "(0.05, 'visual_location', 'CLEARED')\n",
      "(0.05, 'visual_location', \"ENCODED LOCATION:'_visuallocation(color= None, screen_x= 320, screen_y= 180, value= None)'\")\n",
      "(0.05, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.05, 'PROCEDURAL', 'RULE SELECTED: attend word')\n",
      "(0.1, 'PROCEDURAL', 'RULE FIRED: attend word')\n",
      "(0.1, 'g', 'MODIFIED')\n",
      "(0.1, 'visual_location', 'CLEARED')\n",
      "(0.1, 'visual', 'PREPARATION TO SHIFT VISUAL ATTENTION STARTED')\n",
      "(0.1, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.1, 'PROCEDURAL', 'NO RULE FOUND')\n",
      "(0.1163, 'visual', 'CLEARED')\n",
      "(0.1163, 'visual', \"ENCODED VIS OBJECT:'_visual(cmd= move_attention, color= , screen_pos= _visuallocation(color= None, screen_x= 320, screen_y= 180, value= None), value= elephant)'\")\n",
      "(0.1163, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.1163, 'PROCEDURAL', 'RULE SELECTED: retrieving')\n",
      "(0.1663, 'PROCEDURAL', 'RULE FIRED: retrieving')\n",
      "(0.1663, 'g', 'MODIFIED')\n",
      "(0.1663, 'retrieval', 'START RETRIEVAL')\n",
      "(0.1663, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.1663, 'PROCEDURAL', 'NO RULE FOUND')\n",
      "(0.1916, 'visual', 'PREPARATION TO SHIFT VISUAL ATTENTION COMPLETED')\n",
      "(0.1916, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.1916, 'PROCEDURAL', 'NO RULE FOUND')\n",
      "(0.2163, 'retrieval', 'CLEARED')\n",
      "(0.2163, 'retrieval', 'RETRIEVED: word(form= elephant)')\n",
      "(0.2163, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.2163, 'PROCEDURAL', 'RULE SELECTED: lexeme retrieved')\n",
      "(0.2663, 'PROCEDURAL', 'RULE FIRED: lexeme retrieved')\n",
      "(0.2663, 'g', 'MODIFIED')\n",
      "(0.2663, 'manual', 'COMMAND: press_key')\n",
      "(0.2663, 'manual', 'PREPARATION COMPLETE')\n",
      "(0.2663, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.2663, 'PROCEDURAL', 'NO RULE FOUND')\n",
      "(0.3162, 'visual', 'SHIFT COMPLETE TO POSITION: [320, 180]')\n",
      "(0.3162, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.3162, 'PROCEDURAL', 'NO RULE FOUND')\n",
      "(0.3163, 'manual', 'INITIATION COMPLETE')\n",
      "(0.3163, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.3163, 'PROCEDURAL', 'NO RULE FOUND')\n",
      "(0.4163, 'manual', 'KEY PRESSED: J')\n",
      "(0.4163, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.4163, 'PROCEDURAL', 'NO RULE FOUND')\n",
      "(0.5663, 'manual', 'MOVEMENT FINISHED')\n",
      "(0.5663, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.5663, 'PROCEDURAL', 'NO RULE FOUND')\n"
     ]
    }
   ],
   "source": [
    "lex_dec_sim.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we see that it should take a bit more than $400$ ms to find a stimulus, decide whether it is a word and press the right key -- see the event ```'KEY PRESSED: J'``` above\n",
    "- this is slightly faster than the $500-600$ ms usually found in lexical decision tasks, but it is a consequence of our inadequate modeling of memory retrieval; consult the papers below for more details about human reaction time in lexical decision tasks:\n",
    "    - Forster, Kenneth I. 1990. Lexical processing. In _Language: An invitation to cognitive science_, ed. Daniel Osherson and Howard Lasnik, 95–131. Cambridge, MA: MIT Press.\n",
    "    - Murray, Wayne S, and Kenneth I. Forster. 2004. Serial mechanisms in lexical access: the rank hypothesis. _Psychological Review_ 111:721\n",
    "- while visual and motor processes are fairly realistically modeled, we assume retrieval always takes $50$ ms regardless of the specific features of the word we're trying to retrieve and the cognitive state in which retrieval happens\n",
    "- we will address this when we introduce the sub-symbolic components of ACT-R in future notebooks, which we will then use to model lexical decision tasks much more realistically\n",
    "\n",
    "The remainder of this notebook is dedicated to discussing the visual and manual processes that are chronicled in the output of the simulation in the trace above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual processes in our lexical decision model\n",
    "\n",
    "Traditionally, visual attention is equated to (keeping track of) the focus position of the eyes:\n",
    "\n",
    "- Just, Marcel A., and Patricia A. Carpenter. 1980. A theory of reading: From eye fixations to comprehension. _Psychological Review_ 87:329–354\n",
    "- Just, Marcel A., Patricia A. Carpenter, and Jacqueline D. Woolley. 1982. Paradigms and processes in reading comprehension. _Journal of Experimental Psychology: General_ 111:228–238\n",
    "\n",
    "- understanding which word one attends to is tantamount to identifying which word the eyes are focused on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this identification of the unobservable cognitive state (attention) and overt behavior (eye focus position) is an overly simplified model.\n",
    "\n",
    "- for example, it is known that when people read, some words, especially high-frequency ones, are processed without ever receiving eye focus\n",
    "    - Schilling, Hildur EH, Keith Rayner, and James I Chumbley. 1998. Comparing naming, lexical decision, and eye fixation times: Word frequency effects and individual differences. _Memory and Cognition_ 26:1270–1281\n",
    "    - Rayner, Keith. 1998. Eye movements in reading and information processing: 20 years of research. _Psychological Bulletin_ 124:372–422"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EMMA model (Salvucci, Dario D. 2001. An integrated model of eye movements and visual encoding. _Cognitive Systems Research_ 1:201–220) incorporated in ACT-R and implemented in ```pyactr``` captures this by disassociating eye focus and attention:\n",
    "\n",
    "- the two processes are related but not identical\n",
    "- a shift of attention to a visual object, for example, the command ```move_attention``` on line 15 above triggers:\n",
    "   - an immediate attempt to encode the object as an internal representation, and at the same time,\n",
    "   - eye movement\n",
    "- but the two processes proceed independently of each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first discuss the process of encoding a visual object.\n",
    "\n",
    "The time $t_{\\textit{enc}}$ needed to encode an object is modeled using a Gamma distribution (a generalization of the exponential distribution) with mean $T_{\\textit{enc}}$ and standard deviation one third of the mean.\n",
    "\n",
    "- $t_{\\textit{enc}}\\sim Gamma(\\mu=T_{\\textit{enc}}, \\sigma=T_{\\textit{enc}}/3)$\n",
    "- $T_{\\textit{enc}}=K\\cdot (-\\log{f}) \\cdot e^{kd}$, where:\n",
    "    - $f$ is the (normalized) frequency of the object (word) being encoded\n",
    "    - $d$ is the distance between the current focal point of the eyes and the object to be encoded measured in degrees of visual angle ($d$ is the eccentricity of the object relative to the current eye position)\n",
    "        - that is, the parameter $T_{\\textit{enc}}$ is crucially parametrized by the distance $d$ between the current eye focus position and the position of the target object\n",
    "    - $k$ is a free parameter, scaling the effect of distance (set to $1$ by default)\n",
    "    - $K$ is a free parameter, scaling the encoding time itself (set to $0.01$ by default)\n",
    "\n",
    "In the trace of the simulation, the time point of encoding a visual object is signaled by the event ```ENCODED VIS OBJECT```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note about parametrizing Gamma distributions**\n",
    "\n",
    "Gamma distributions are usually parametrized in terms of a shape $\\alpha$ and a rate $\\beta$ or scale $\\frac{1}{\\beta}$. We can convert our non-standard parametrization into the standard one(s) as follows: shape $\\alpha = (\\frac{\\mu}{\\sigma})^2$ and rate $\\beta = \\frac{\\mu}{\\sigma^2}$ (equivalently: scale $\\frac{1}{\\beta} = \\frac{\\sigma^2}{\\mu}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us turn now to discussing the eye movement process.\n",
    "\n",
    "The time needed for eye movement to the new object is split into two sub-processes:\n",
    "\n",
    "- preparation, and\n",
    "- execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preparation is modeled once again as a Gamma distribution with mean $135$ ms and a standard deviation of $45$ ms (yet again, the standard deviation is one third of the mean)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution, which follows the preparation, is also modeled as a Gamma distribution with:\n",
    "\n",
    "- a mean of $70$ ms + $2$ ms for every degree of visual angle between the current eye position and the targeted visual object, and\n",
    "- a standard deviation that is one third of the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is only at the end of the execution sub-process that the eyes focus on the new position.\n",
    "\n",
    "- thus, the whole process of eye movement takes around $200$ ms ($\\approx 135 + 70$), which corresponds to average saccade latencies reported in previous studies (see, e.g., Fuchs, Albert. 1971. The saccadic system. _The control of eye movements_ 343–362)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our simulation:\n",
    "\n",
    "- the event ```PREPARATION TO SHIFT VISUAL ATTENTION COMPLETED``` signals the end of the preparation phase\n",
    "- the end of the execution phase is signaled by ```SHIFT COMPLETE TO POSITION [320, 180]```\n",
    "\n",
    "It is only at this point that the eyes focus on the new location, but the internal representation of the object has already been encoded:\n",
    "\n",
    "- the word has already been retrieved from memory by this point, as indicated by the earlier event ```RETRIEVED: word(form= elephant)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the two processes of visual encoding and eye movement interact?\n",
    "\n",
    "- one possibility is that encoding is done before the end of the preparation phase \n",
    "    - this is actually the case in our trace above\n",
    "    - when this happens, the planned eye movement can be canceled, but only if the cognitive processes following visual encoding are fast enough to\n",
    "        - cancel the eye shift, or\n",
    "        - request a new eye-focus position before the end of the preparation phase\n",
    "\n",
    "- the second possibility is that visual encoding is finished only during the execution phase of the eye movement process\n",
    "    - in this case, eye movement cannot be stopped anymore and the eye shift is actually carried out\n",
    "\n",
    "- the third and final possibility is that visual encoding is still not done after the eyes shift to a new position\n",
    "    - in this case, visual encoding is restarted\n",
    "    - since the eyes have moved closer to the position of the object we're trying to encode, the time necessary for visual encoding is now decreased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how the restarted visual encoding time is decreased, consider what the new encoding time would have been if this had been an initial visual encoding.\n",
    "\n",
    "- we would have a random draw $t'_{\\textit{enc}}$ from a Gamma distribution centered at a new mean $T'_{\\textit{enc}}$ because the distance between the object and the new position of the eyes has now changed to $d'$\n",
    "    - $t'_{\\textit{enc}}\\sim Gamma(\\mu=T'_{\\textit{enc}}, \\sigma=T'_{\\textit{enc}}/3)$\n",
    "    - $T'_{\\textit{enc}}=K\\cdot (-\\log{f}) \\cdot e^{kd'}$\n",
    "\n",
    "But instead of taking the full $t'_{\\textit{enc}}$ time to do the visual encoding, we will scale that down by the amount of time we already spent during our initial encoding attempt:\n",
    "\n",
    "- we will look at the initial expected encoding time $t_{\\textit{enc}}$ and at the time $t_{\\textit{completed}}$ that we actually spent encoding\n",
    "    - necessarily, $t_{\\textit{completed}} < t_{\\textit{enc}}$\n",
    "- we can say that we have already completed a percentage of the visual encoding process, and that percentage is $\\frac{t_{\\textit{completed}}}{t_{\\textit{enc}}}$\n",
    "- the new processing time should be the remaining percentage that we have not completed yet, i.e.:\n",
    "    - $\\frac{t_{\\textit{enc}} - t_{\\textit{completed}}}{t_{\\textit{enc}}}$, or equivalently\n",
    "    - $1 - \\frac{t_{\\textit{completed}}}{t_{\\textit{enc}}}$\n",
    "- thus, instead of saying that the new encoding time is the full $t'_{\\textit{enc}}$, we will only need the percentage of it that is the same as the percentage of incomplete processing we had left after our first encoding attempt\n",
    "    - visual reencoding time: $\\left(1 - \\frac{t_{\\textit{completed}}}{t_{\\textit{enc}}}\\right) \\cdot t'_{\\textit{enc}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual processes in our lexical decision model\n",
    "\n",
    "Similarly to the vision process, the motor process is split into several sub-phases when carrying out a command:\n",
    "\n",
    "- the preparation phase\n",
    "- the initiation phase\n",
    "- the actual key press\n",
    "- finishing the movement (returning to the original position)\n",
    "\n",
    "As in the case of the visual module, cognitive processes can interrupt a movement, but only during the preparation phase.\n",
    "\n",
    "The time needed to carry out every phase is dependent on several variables:\n",
    "\n",
    "- is this the first movement or not? if a key was pressed before, was it pressed with the same hand or not?\n",
    "    - answers to these questions influence the amount of time the preparation phase takes\n",
    "- is the key to be pressed on the home row or not?\n",
    "    - the answer to this question influences the amount of time the actual movement requires, as well as the preparation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
