{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](./images/colab-badge.png)](https://colab.research.google.com/github/abrsvn/pyactr-book/blob/master/notebooks/8_top_down_parsing_model_structure_and_dec_mem.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-down parsing\n",
    "\n",
    "Now that the basic ACT-R cognitive architecture is in place and we're more familiar with its specific implementation in ```pyactr```, let us build a basic model of syntactic parsing. Specifically, we will build a top-down parser, i.e., a parser that uses the grammar to make predictions about the sentential structure of the upcoming input.\n",
    "\n",
    "There are three properties of the human parser that we want our model to capture:\n",
    "\n",
    "- the parser is _incremental_: syntactic parsing and semantic interpretation do not lag significantly behind the perception of individual words;\n",
    "- the parser is _predictive_: the processor forms explicit representations of words and phrases that have not yet been heard;\n",
    "- finally, the parser _satisfies the competence hypothesis_: understanding a sentence / discourse involves the recovery of the structural description of that sentence / discourse on the syntax side, and of the meaning representation on the semantic side.\n",
    "\n",
    "Some references:\n",
    "- Marslen-Wilson, William. 1973. Linguistic structure and speech shadowing at very short latencies. _Nature_ 244:522–523\n",
    "- Frazier, Lyn, and Janet Dean Fodor. 1978. The sausage machine: A new two-stage parsing model. _Cognition_ 6:291–325\n",
    "- Tanenhaus, M. K., M. J. Spivey-Knowlton, K. M. Eberhard, and J. C. Sedivy. 1995. Integration of visual and linguistic information in spoken language comprehension. _Science_ 268:1632–1634\n",
    "- Steedman, Mark. 2001. _The syntactic process_. Cambridge, MA: MIT Press\n",
    "- Hale, John. 2011. What a rational parser would do. _Cognitive Science_ 35:399–443"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A top-down parser satisfies these conditions, and it has the pedagogical advantage of being very simple (too simple, in fact, to be cognitively plausible). So let's start with one.\n",
    "\n",
    "Suppose we have a simplecontext-free grammar with the following rules:\n",
    "\n",
    "$$\\begin{array}[t]{lcl}\n",
    "        \\mbox{S}&\\rightarrow&\\mbox{NP } \\mbox{VP}\\\\\n",
    "        \\mbox{NP}&\\rightarrow&\\mbox{ProperN}\\\\\n",
    "        \\mbox{VP}&\\rightarrow&\\mbox{V } \\mbox{NP}\\\\\n",
    "        \\mbox{ProperN}&\\rightarrow&\\mbox{Mary}\\\\\n",
    "        \\mbox{ProperN}&\\rightarrow&\\mbox{Bill}\\\\\n",
    "        \\mbox{V}&\\rightarrow&\\mbox{likes}\n",
    "  \\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that we have only\n",
    "- two proper names\n",
    "- one transitive verb\n",
    "\n",
    "Our goal is to build a top-down parser that is able to analyze the sentence _Mary likes Bill_.\n",
    "\n",
    "We assume the sentence is presented to the comprehender one word at a time in the manner of self-paced reading tasks (Just, Marcel A., Patricia A. Carpenter, and Jacqueline D. Woolley. 1982. Paradigms and pro-\n",
    "cesses in reading comprehension. _Journal of Experimental Psychology: General_ 111:228–238).\n",
    "\n",
    "In such tasks, the words are hidden and only one word is uncovered at a time with a spacebar press. The human reader decides when to press the spacebar to uncover the next word (which automatically hides the current word), hence the name of self-paced reading.\n",
    "\n",
    "So reading our sentence _Mary likes Bill_ will happen in four successive stages. In one such version of self-paced reading (the so-called non-cumulative moving-window paradigm), the whole process would look as shown below.\n",
    "\n",
    "\n",
    "- initial display:\n",
    "\n",
    "```---- ----- ---```\n",
    "\n",
    "- after one spacebar press:\n",
    "\n",
    "```Mary ----- ---```\n",
    "\n",
    "- after another spacebar press:\n",
    "\n",
    "```---- likes ---```\n",
    "\n",
    "- after the third spacebar press:\n",
    "\n",
    "```---- ----- Bill```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-paced reading tasks mimic an essential aspect of naturally-occurring language comprehension with auditory stimuli:\n",
    "\n",
    "- the signal is strictly linearly and strictly incrementally presented one word at a time\n",
    "\n",
    "Just as in naturally-occurring verbal interactions, and unlike in normal reading situations, the linguistic signal cannot be 'rewound' to previous words\n",
    "\n",
    "- we cannot just look back and reread previous parts of the text\n",
    "\n",
    "or  'fast-forwarded' to subsequent words\n",
    "\n",
    "- we cannot jump ahead to parts of the text that do not immediately follow the word currently being read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can proceed to the characterization of our processing model. A top-down parser can be thought of as a push-down automaton, i.e., an automaton that has a basic form of memory represented as a **stack**.\n",
    "\n",
    "- the stack stores parsing goals and subgoals in a strict, total order\n",
    "- these goals are accomplished one at a time by accessing the top of the stack\n",
    "\n",
    "In our case, the parsing goals are simply syntactic categories that have to be parsed, i.e., that have to be identified in the incoming string.\n",
    "\n",
    "For example, when we start the parsing process, we push the initial goal of parsing an S node onto the stack. The stack has now only one goal in it, namely 'parse an S', and the goal sits at the top of the stack.\n",
    "\n",
    "$$\\begin{array}{|c|}\\hline\n",
    "        S\\\\\\hline\n",
    "  \\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pop goals off the stack one at a time: we can only look at the top of the stack and remove the current top goal when this goal is accomplished or broken down exhaustively into subgoals.\n",
    "\n",
    "For example, we will pop the 'parse an S' goal off the stack when we apply the first grammar rule above and replace this goal with two subgoals:\n",
    "\n",
    "- first parse an NP (i.e., identify an NP in the incoming word input)\n",
    "- then parse a VP.\n",
    "\n",
    "The resulting stack will now have two goals: the top one is 'parse an NP', and the one below it is 'parse a VP':\n",
    "\n",
    "$$\\begin{array}{|c|}\\hline\n",
    "        S\\\\\\hline\n",
    "    \\end{array}\n",
    "    \\quad\\Rightarrow\\quad\n",
    "    \\begin{array}{|c|}\\hline\n",
    "        NP\\\\\\hline\n",
    "        VP\\\\\\hline\n",
    "    \\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parser works by modifying the contents of its stack based on two pieces of information: the top element on the stack and, possibly, the current word that has to be parsed (the leftmost word in the incoming string of words).\n",
    "\n",
    "We can sum up top-down parsing as a parsing strategy that applies two algorithm schemata, _expand_ and _scan_, in this order (see, for example, Hale, John T. 2014. _Automaton theories of human sentence comprehension_. Stanford: CSLI Publications, for a good, detailed introduction):\n",
    "\n",
    "**Top-down parsing rules**:\n",
    "\n",
    "- __expand__:\n",
    "    - if the stack has a symbol $X$ on top, and the grammar contains a rule $X \\rightarrow A\\mbox{ }B$ or $X \\rightarrow A$, then:\n",
    "        - pop $X$ and push down onto the stack the symbols $B$ and $A$ (in that order), or\n",
    "        - pop $X$ and push down onto the stack the symbol $A$.\n",
    "- __scan__:\n",
    "    - if the top of stack has a terminal symbol -- a symbol like _V_ or _ProperN_ that rewrites to a lexical item; that is, a part of speech -- and $w$, the leftmost word to be parsed, is of that part of speech, then:\n",
    "        - pop the terminal symbol off the stack and remove $w$ from the word string that is to be parsed.\n",
    "\n",
    "Let us now code a top-down parser in ```pyactr``` that implements these two general parsing rules and uses the grammar above. Recall that the example sentence we will parse is _Mary likes Bill_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a top-down parser in ```pyactr``` [Part 1]\n",
    "\n",
    "Let us start with the first standard step, importing ```pyactr```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyactr as actr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We should now specify the types of chunks we need. We will have one type for parsing goals. The parsing goal will keep track of:\n",
    "\n",
    "- the stack content: we only need two positions in the stack for our current purposes -- the top and the bottom of the stack; this is a consequence of the fact that our grammar generates at most binary branching trees with no left recursion (cf. Resnik, Philip. 1992. Left-corner parsing and psychological plausibility. In _Proceedings of the Fourteenth International Conference on Computational Linguistics_. Nantes, France);\n",
    "- the current word being parsed (if any);\n",
    "- the current task of the parser, that is, the current state our parsing model is in -- basically, 'parsing' if the parse is still ongoing, and 'done' if the parsing is finished.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actr.chunktype(\"parsing_goal\", \"stack_top stack_bottom parsed_word task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second chunk type we need to declare is one that will enable us to represent the incoming sentence, i.e., the incoming word string, to be parsed.\n",
    "\n",
    "This might seem counter-intuitive: why should we represent the sentence to be parsed in a chunk? The sentence is external to the agent, it's what the agent reads or hears.\n",
    "\n",
    "However, at this point, we have no way of representing the surrounding environment and the basic input/output interfaces between the mind and the environment. We therefore have to represent a sentence internally as a chunk.\n",
    "\n",
    "When we introduce the vision and motor modules in future notebooks (associated with Chapter 4 of the _Computational Cognitive Modeling and Linguistics Theory_ book), we will be able to develop a more intuitive and elegant solution.\n",
    "\n",
    "The chunk type for sentences only needs to store three words since our target sentence is only that long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actr.chunktype(\"sentence\", \"word1 word2 word3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules, buffers, and the lexicon\n",
    "\n",
    "Let us now initialize the model and set up more convenient ways of accessing the declarative memory module and the goal buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = actr.ACTRModel()\n",
    "dm = parser.decmem\n",
    "g = parser.goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal buffer will store a ```parsing_goal``` chunk, which:\n",
    "\n",
    "- carries the information that drives the parsing process, and\n",
    "- is updated throughout that process.\n",
    "\n",
    "But we also need to store the word sequence that we need to parse, so we will create a second buffer that is similar to the goal buffer and that will store the sentence to be parsed.\n",
    "\n",
    "Having two goal-like buffers is not uncommon in ACT-R.\n",
    "\n",
    "- the first buffer is the actual goal buffer, which keeps track of the information driving the cognitive process\n",
    "- the other one is the _imaginal_ buffer; this buffer:\n",
    "    - is associated with the imaginal module\n",
    "    - maintains an internal image of the information associated with the current cognitive process that provides contextual information relevant for the current task.\n",
    "    \n",
    "Thus, storing the sentence to be parsed in the imaginal buffer is an acceptable approximation of the cognitive behavior we're trying to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaginal = parser.set_goal(name=\"imaginal\", delay=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we create a new goal buffer, which we call the ```imaginal``` buffer.\n",
    "\n",
    "- the string ```\"imaginal\"``` sets the name under which the model will recognize and access the buffer (e.g., in production rules)\n",
    "- the ```delay``` attribute of the imaginal buffer encodes the delay required to set a chunk in the buffer:\n",
    "    - it will take $0.2$ seconds ($200$ ms) to set a chunk in the ```imaginal``` buffer\n",
    "    - this is the standard value for this buffer, in contrast to the ```goal``` buffer, which sets a chunk immediately\n",
    "    \n",
    "Finally, we assign this new buffer to a variable ```imaginal``` so that we can access it more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal and imaginal buffers, and more generally the state of the buffers at any given point in a cognitive process provides the internal state, or the context, of the cognitive process at that point.**\n",
    "\n",
    "For example, items in memory that share values with items in the goal or imaginal buffers are contextually 'primed':\n",
    "\n",
    "- they are more salient (technically, more activated) than other items\n",
    "- they are therefore easier to retrieve, precisely because they are relevant in context.\n",
    "\n",
    "Thus, the cognitive context in the sense of **the current state of the buffers** has a function similar to variable assignments in first-order logic.\n",
    "\n",
    "- assignments in first-order logic provide the current context of interpretation relative to which incoming expressions are interpreted\n",
    "- similarly, the state of the buffers in an ACT-R model of the mind provide the context for the next step in the cognitive process.\n",
    "\n",
    "Incidentally, the counterpart of the model in first-order logic is the content of the modules, particularly the facts stored in declarative memory and the rules stored in procedural memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add chunks to the ```goal``` and ```imaginal``` buffers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{parsing_goal(parsed_word= , stack_bottom= , stack_top= S, task= parsing)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.add(actr.chunkstring(string=\"\"\"\n",
    "    isa parsing_goal\n",
    "    task parsing\n",
    "    stack_top S\n",
    "\"\"\"))\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the ```goal``` buffer switches to an active ```parsing``` state / ```task```, and\n",
    "- the current parsing goal, i.e., the top of the stack, is set to parsing a sentence (```S```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{sentence(word1= Mary, word2= likes, word3= Bill)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imaginal.add(actr.chunkstring(string=\"\"\"\n",
    "    isa sentence\n",
    "    word1 Mary\n",
    "    word2 likes\n",
    "    word3 Bill\n",
    "\"\"\"))\n",
    "\n",
    "imaginal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in the ```imaginal``` buffer, we set the ```sentence``` to be parsed to _Mary likes Bill_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start answering our main question: **how do we code the top-down parser itself?**\n",
    "\n",
    "We will assume that the grammar and associated parsing rules are part of the ```procedural``` module, i.e., they are encoded by production rules.\n",
    "\n",
    "- this contrasts with lexical information, which is commonly encoded in declarative memory\n",
    "- see **Lewis, Richard, and Shravan Vasishth. 2005. An activation-based model of sentence process-\n",
    "ing as skilled memory retrieval. _Cognitive Science_ 29:1–45** for more discussion and arguments for this division of labor between the lexicon and grammar.\n",
    "\n",
    "We specify our lexicon first. For simplicity, our lexical representations will encode only:\n",
    "\n",
    "- the form (represented as the standard English spelling of the lexeme)\n",
    "- the part of speech (syntactic category) tags of our lexical items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{word(cat= ProperN, form= Mary): array([0.]), word(cat= ProperN, form= Bill): array([0.]), word(cat= V, form= likes): array([0.])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actr.chunktype(\"word\", \"form, cat\")\n",
    "\n",
    "dm.add(actr.chunkstring(string=\"\"\"\n",
    "    isa word\n",
    "    form Mary\n",
    "    cat ProperN\n",
    "\"\"\"))\n",
    "dm.add(actr.chunkstring(string=\"\"\"\n",
    "    isa word\n",
    "    form Bill\n",
    "    cat ProperN\n",
    "\"\"\"))\n",
    "dm.add(actr.chunkstring(string=\"\"\"\n",
    "    isa word\n",
    "    form likes\n",
    "    cat V\n",
    "\"\"\"))\n",
    "\n",
    "dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
