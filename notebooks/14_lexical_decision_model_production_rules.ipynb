{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](./images/colab-badge.png)](https://colab.research.google.com/github/abrsvn/pyactr-book/blob/master/notebooks/14_lexical_decision_model_production_rules.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model up to this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyactr as actr\n",
    "\n",
    "environment = actr.Environment(focus_position=(0,0))\n",
    "lex_decision = actr.ACTRModel(\n",
    "    environment=environment,\n",
    "    automatic_visual_search=False,\n",
    "    motor_prepared=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actr.chunktype(\"goal\", \"state\")\n",
    "actr.chunktype(\"word\", \"form\")\n",
    "\n",
    "dm = lex_decision.decmem\n",
    "\n",
    "for string in {\"elephant\", \"dog\", \"crocodile\"}:\n",
    "    dm.add(actr.makechunk(typename=\"word\", form=string))\n",
    "\n",
    "g = lex_decision.goal\n",
    "\n",
    "g.add(actr.makechunk(nameofchunk=\"beginning\",\n",
    "                     typename=\"goal\",\n",
    "                     state=\"start\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The lexical decision model: productions\n",
    "\n",
    "We only need five productions to model our lexical decision task. \n",
    "\n",
    "- the first rule requires the visual _Where_ buffer to search the (virtual) screen and find the closest word relative to the starting $(0, 0)$ position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'=g': goal(state= start), '?visual_location': {'buffer': 'empty'}}\n",
       "==>\n",
       "{'=g': goal(state= attend), '+visual_location': _visuallocation(color= , screen_x= closest, screen_y= , value= )}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_decision.productionstring(name=\"find word\", string=\"\"\"\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   start\n",
    "    ?visual_location>\n",
    "    buffer  empty\n",
    "    ==>\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   attend\n",
    "    +visual_location>\n",
    "    isa _visuallocation\n",
    "    screen_x closest\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rule requires:\n",
    "\n",
    "- the ```start``` chunk to be in the goal buffer (lines 2-4 above)\n",
    "- the visual location buffer to be empty (lines 5-6)\n",
    "\n",
    "If these preconditions are met:\n",
    "\n",
    "- we enter a new goal state of 'attending' to the visual input (lines 8-10)\n",
    "- the visual location buffer will search for and be updated with the position of the closest element (lines 11-13)\n",
    "    - the search is launched by specifying `+visual_location`, that is, the name of the buffer and the task ```+```\n",
    "    - we used ```+``` before for the retrieval buffer when we placed a new retrieval request, i.e., we launched a new search in declarative memory\n",
    "    - we can think of ```+``` in the visual _Where_ buffer as specifying the same action as in the retrieval buffer, the only difference being that\n",
    "        - the visual _Where_ buffer searches the environment\n",
    "        - the retrieval buffer searches the declarative memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this rule fires, our ACT-R model will know the position of the closest element on the screen, but it won't know which element is actually present at that location.\n",
    "\n",
    "To access the element, we make use of the visual _What_ buffer, as shown in the ```\"attend word\"``` rule below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'=g': goal(state= attend), '=visual_location': _visuallocation(color= , screen_x= , screen_y= , value= ), '?visual': {'state': 'free'}}\n",
       "==>\n",
       "{'=g': goal(state= retrieving), '+visual': _visual(cmd= move_attention, color= , screen_pos= =visual_location, value= ), '~visual_location': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_decision.productionstring(name=\"attend word\", string=\"\"\"\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   attend\n",
    "    =visual_location>\n",
    "    isa    _visuallocation\n",
    "    ?visual>\n",
    "    state   free\n",
    "    ==>\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   retrieving\n",
    "    +visual>\n",
    "    isa     _visual\n",
    "    cmd     move_attention\n",
    "    screen_pos =visual_location\n",
    "    ~visual_location>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rule checks that:\n",
    "\n",
    "- the visual _Where_ buffer has stored a location (lines 5-6)\n",
    "- the visual _What_ buffer is free, i.e., it is not carrying out any visual action\n",
    "\n",
    "If these preconditions are satisfied:\n",
    "\n",
    "- a new chunk is added to the visual _What_ buffer that moves the focus of attention to the current visual location (lines 13-16)\n",
    "- the attention focus is moved by setting the value of the ```cmd``` (command) slot to ```move_attention```\n",
    "- the goal enters a ```retrieving``` state (lines 10-12)\n",
    "- the visual _Where_ buffer (a.k.a. ```visual_location```) is cleared (line 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interaction between the two vision buffers simulates a two-step process:\n",
    "\n",
    "1. noticing an object through the visual location (_Where_) buffer\n",
    "2. finding what that object is, i.e., attending to the object through the visual (_What_) buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next rule starts the memory retrieval process:\n",
    "\n",
    "- we take the ```value =val``` of the chunk stored in the visual (_What_) buffer, which is a string, and check to see if there is a word in our lexicon that has that form\n",
    "- this retrieval request is actually the core part of our lexical decision model\n",
    "- the crucial parts of the rule are on lines 7 and 14 below:\n",
    "    - the character string ```=val``` of the perceived chunk (line 7) becomes the declarative memory cue placed in the retrieval buffer (line 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'=g': goal(state= retrieving), '=visual': _visual(cmd= , color= , screen_pos= , value= =val)}\n",
       "==>\n",
       "{'=g': goal(state= retrieval_done), '+retrieval': word(form= =val)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_decision.productionstring(name=\"retrieving\", string=\"\"\"\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   retrieving\n",
    "    =visual>\n",
    "    isa     _visual\n",
    "    value   =val\n",
    "    ==>\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   retrieval_done\n",
    "    +retrieval>\n",
    "    isa     word\n",
    "    form    =val\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final two rules we need are provided below. They consider the two possible outcomes of the retrieval process:\n",
    "\n",
    "- a lexeme was retrieved, or\n",
    "- no lexeme was found with that form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'=g': goal(state= retrieval_done), '?retrieval': {'buffer': 'full', 'state': 'free'}}\n",
       "==>\n",
       "{'=g': goal(state= done), '+manual': _manual(cmd= press_key, key= J)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_decision.productionstring(name=\"lexeme retrieved\", string=\"\"\"\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   retrieval_done\n",
    "    ?retrieval>\n",
    "    buffer  full\n",
    "    state   free\n",
    "    ==>\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   done\n",
    "    +manual>\n",
    "    isa     _manual\n",
    "    cmd     press_key\n",
    "    key     J\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'=g': goal(state= retrieval_done), '?retrieval': {'buffer': 'empty', 'state': 'error'}}\n",
       "==>\n",
       "{'=g': goal(state= done), '+manual': _manual(cmd= press_key, key= F)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_decision.productionstring(name=\"no lexeme found\", string=\"\"\"\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   retrieval_done\n",
    "    ?retrieval>\n",
    "    buffer  empty\n",
    "    state   error\n",
    "    ==>\n",
    "    =g>\n",
    "    isa     goal\n",
    "    state   done\n",
    "    +manual>\n",
    "    isa     _manual\n",
    "    cmd     press_key\n",
    "    key     F\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of the rules should look familiar by now. The only new parts are on lines 12-15 (in both rules above):\n",
    "\n",
    "- these lines set the motor module in action, which can perform only one action, namely pressing a key\n",
    "- this is implemented by placing a chunk of a special predefined type ```_manual``` in the ```manual``` buffer\n",
    "- the chunk has two slots:\n",
    "    - ```cmd```: what command should be carried out\n",
    "    - ```key``` : what key should be pressed \n",
    "- the command is the same for both rules (```press_key``` on line 14)\n",
    "- the key to be pressed is different:\n",
    "    - if a lexeme is found, the ACT-R model simulating a participant presses ```'J'``` (line 15)\n",
    "    - otherwise, the model presses ```'F'``` (also line 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
